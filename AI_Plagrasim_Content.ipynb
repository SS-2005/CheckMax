{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dmKAD8DsmCu",
        "outputId": "003ff1d6-e7af-451d-faa3-1b1681c97de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis Summary:\n",
            "Average AI Generation Probability: 33.10%\n",
            "Average Plagiarism Probability: 38.38%\n",
            "\n",
            "Detailed analysis saved to student_responses_analysis.csv\n",
            "\n",
            "Detailed Analysis per Question:\n",
            "\n",
            "Question 1:\n",
            "AI Generated Probability: 0.0%\n",
            "Plagiarism Probability: 3.11%\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 2:\n",
            "AI Generated Probability: 0.0%\n",
            "Plagiarism Probability: 5.89%\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 3:\n",
            "AI Generated Probability: 66.94%\n",
            "Plagiarism Probability: 44.53%\n",
            "--------------------------------------------------\n",
            "\n",
            "Question 4:\n",
            "AI Generated Probability: 65.45%\n",
            "Plagiarism Probability: 100.0%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "from spellchecker import SpellChecker  # For spelling and grammar checks\n",
        "\n",
        "class ResponseAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Load a better AI detection model (e.g., OpenAI's GPT-based model or other reliable models)\n",
        "        self.ai_detector = pipeline(\"text-classification\",\n",
        "                                  model=\"roberta-base-openai-detector\",  # Replace with a better model\n",
        "                                  device=0 if torch.cuda.is_available() else -1)\n",
        "        self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        self.min_word_count = 50  # Minimum word count to consider AI-generated text\n",
        "        self.spell_checker = SpellChecker()  # For spelling and grammar checks\n",
        "\n",
        "    def check_ai_generated(self, text):\n",
        "        \"\"\"Check if text is AI generated, with adjustments for word count, spelling, and grammar.\"\"\"\n",
        "        word_count = len(text.split())\n",
        "\n",
        "        # Check for poor spelling and grammar\n",
        "        misspelled = self.spell_checker.unknown(text.split())\n",
        "        misspelled_count = len(misspelled)\n",
        "\n",
        "        # Determine if the text is grammatically correct and spelled correctly\n",
        "        is_grammatically_correct = misspelled_count == 0  # Assume no misspelled words means grammatically correct\n",
        "\n",
        "        # Conditions for AI-generated text\n",
        "        is_long_text = word_count > self.min_word_count\n",
        "        is_well_written = is_grammatically_correct\n",
        "\n",
        "        # Calculate AI probability based on conditions\n",
        "        if is_long_text and is_well_written:\n",
        "            # If all conditions are met, it's most likely AI-generated\n",
        "            ai_probability = 90.0  # High probability\n",
        "        elif is_long_text or is_well_written:\n",
        "            # If at least one condition is met, assign neutral probability\n",
        "            ai_probability = 50.0  # Neutral probability\n",
        "        else:\n",
        "            # If no conditions are met, it's unlikely to be AI-generated\n",
        "            ai_probability = 0.0  # Low probability\n",
        "\n",
        "        # Use the AI detector to refine the probability if conditions are met\n",
        "        if is_long_text or is_well_written:\n",
        "            result = self.ai_detector(text)\n",
        "            detector_probability = round(result[0]['score'] * 100, 2)\n",
        "            # Blend the detector probability with the condition-based probability\n",
        "            ai_probability = (ai_probability + detector_probability) / 2\n",
        "\n",
        "        # Ensure AI probability is within bounds\n",
        "        ai_probability = max(0, min(100, ai_probability))\n",
        "\n",
        "        return round(ai_probability, 2)\n",
        "\n",
        "    def check_plagiarism(self, student_answer, reference_text):\n",
        "        \"\"\"Check plagiarism against reference text\"\"\"\n",
        "        # Split reference text into chunks\n",
        "        chunks = [s.strip() for s in reference_text.split('.') if s.strip()]\n",
        "\n",
        "        # Get embeddings\n",
        "        student_embedding = self.similarity_model.encode(student_answer, convert_to_tensor=True)\n",
        "        chunk_embeddings = self.similarity_model.encode(chunks, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = util.pytorch_cos_sim(student_embedding, chunk_embeddings)\n",
        "        max_similarity = float(torch.max(similarities)) * 100\n",
        "\n",
        "        return round(max_similarity, 2)\n",
        "\n",
        "def analyze_responses(json_file='student_responses.json'):\n",
        "    # Load the analyzer\n",
        "    analyzer = ResponseAnalyzer()\n",
        "\n",
        "    # Read existing responses\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create lists for DataFrame\n",
        "    records = []\n",
        "\n",
        "    # Analyze each response\n",
        "    for q_data in data['questions']:\n",
        "        ai_score = analyzer.check_ai_generated(q_data['user_answer'])\n",
        "        plagiarism_score = analyzer.check_plagiarism(q_data['user_answer'], q_data['correct_answer'])\n",
        "\n",
        "        record = {\n",
        "            'question': q_data['question'],\n",
        "            'user_answer': q_data['user_answer'],\n",
        "            'correct_answer': q_data['correct_answer'],\n",
        "            'time_taken': q_data['time_taken'],\n",
        "            'similarity_score': q_data['similarity'],\n",
        "            'marks': q_data['marks'],\n",
        "            'ai_generated_probability': ai_score,\n",
        "            'plagiarism_probability': plagiarism_score\n",
        "        }\n",
        "        records.append(record)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_filename = 'student_responses_analysis.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    # Update JSON with new metrics\n",
        "    for i, q_data in enumerate(data['questions']):\n",
        "        q_data['ai_generated_probability'] = float(records[i]['ai_generated_probability'])\n",
        "        q_data['plagiarism_probability'] = float(records[i]['plagiarism_probability'])\n",
        "\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nAnalysis Summary:\")\n",
        "    print(f\"Average AI Generation Probability: {df['ai_generated_probability'].mean():.2f}%\")\n",
        "    print(f\"Average Plagiarism Probability: {df['plagiarism_probability'].mean():.2f}%\")\n",
        "    print(f\"\\nDetailed analysis saved to {csv_filename}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    df = analyze_responses()\n",
        "\n",
        "    # Display sample analysis\n",
        "    print(\"\\nDetailed Analysis per Question:\")\n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"\\nQuestion {idx + 1}:\")\n",
        "        print(f\"AI Generated Probability: {row['ai_generated_probability']}%\")\n",
        "        print(f\"Plagiarism Probability: {row['plagiarism_probability']}%\")\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VD2f1LzyssYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}